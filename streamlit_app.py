import os
import streamlit as st
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import PyPDFLoader
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.tools.retriever import create_retriever_tool
from langchain.prompts import ChatPromptTemplate
import tempfile
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain_community.utilities import SerpAPIWrapper
from langchain.agents import Tool
from typing import List
from langchain_community.document_loaders import WebBaseLoader, YoutubeLoader, CSVLoader
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever
# fallbackìš©
# from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound, CouldNotRetrieveTranscript
# import yt_dlp
# import tempfile, os

# .env íŒŒì¼ ë¡œë“œ
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

# ê³µí†µ: ë¬¸ì„œ â†’ ë²¡í„°DB â†’ (ì„ íƒ) ì••ì¶• ë¦¬íŠ¸ë¦¬ë²„
def _build_retriever_from_documents(documents, chunk_size: int = 1000, chunk_overlap: int = 200, compress: bool = True):
    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    splits = splitter.split_documents(documents)
    vector = FAISS.from_documents(splits, OpenAIEmbeddings())
    base = vector.as_retriever(search_kwargs={"k": 6})

    if not compress:
        return base

    # ì¿¼ë¦¬ ê´€ë ¨ í•µì‹¬ë§Œ ì¶”ì¶œí•˜ëŠ” ì••ì¶• ë¦¬íŠ¸ë¦¬ë²„
    llm_for_compress = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)
    compressor = LLMChainExtractor.from_llm(llm_for_compress)
    return ContextualCompressionRetriever(base_compressor=compressor, base_retriever=base)

# ì›¹í˜ì´ì§€ RAG íˆ´
def create_website_rag_tool(urls: List[str]):
    urls = [u.strip() for u in urls if u and u.strip()]
    if not urls:
        return None
    loader = WebBaseLoader(urls)
    docs = loader.load()
    retriever = _build_retriever_from_documents(docs, compress=True)
    return create_retriever_tool(
        retriever,
        name="website_search",
        description="RAG over provided web pages. ê³µì‹ ë¬¸ì„œ/ë¸”ë¡œê·¸/ìœ„í‚¤ ë“± URL ê·¼ê±° ê¸°ë°˜ ê²€ìƒ‰ì— ì‚¬ìš©."
    )

# ìœ íŠœë¸Œ ìë§‰ RAG íˆ´
def create_youtube_rag_tool(video_urls: list[str]):
    video_urls = [u.strip() for u in (video_urls or []) if u and u.strip()]
    if not video_urls:
        return None

    all_docs, errors = [], []

    def _load_with_langchain(url: str):
        loader = YoutubeLoader.from_youtube_url(
            url,
            add_video_info=True,
            language=["ko", "en"],   # ìš°ì„  ko, ë‹¤ìŒ en
            translation="ko",        # ë²ˆì—­ìë§‰ í—ˆìš©
        )
        return loader.load()

    def _load_with_transcript_api(url: str):
        # youtube_transcript_api ì§ì ‘ í˜¸ì¶œ
        vid = url.split("v=", 1)[1].split("&", 1)[0]
        tr_list = YouTubeTranscriptApi.list_transcripts(vid)

        # ko ìš°ì„  â†’ en â†’ enì„ ko ë²ˆì—­
        try:
            tr = tr_list.find_transcript(["ko"])
        except Exception:
            tr = None
        if not tr:
            try:
                tr = tr_list.find_transcript(["en"])
            except Exception:
                tr = None
        fetched = None
        if tr:
            try:
                fetched = tr.fetch()
            except Exception:
                fetched = None

        if not fetched:
            # en â†’ ko ë²ˆì—­ ì‹œë„
            try:
                tr = tr_list.find_transcript(["en"]).translate("ko")
                fetched = tr.fetch()
            except Exception:
                fetched = None

        if not fetched:
            raise NoTranscriptFound(vid)

        text = "\n".join([i["text"] for i in fetched if i.get("text")])
        lang = (tr.language_code if hasattr(tr, "language_code") else "ko")
        return [Document(page_content=text, metadata={"source": url, "language": lang, "loader": "youtube_transcript_api"})]

    def _load_with_ytdlp(url: str):
        # ìë™ìƒì„±/ì—…ë¡œë” ì œê³µ ìë§‰ íŒŒì¼ì„ ë‚´ë ¤ë°›ì•„ íŒŒì‹±
        with tempfile.TemporaryDirectory() as tmpdir:
            ydl_opts = {
                "skip_download": True,
                "writesubtitles": True,
                "writeautomaticsub": True,
                "subtitleslangs": ["ko", "en"],
                "outtmpl": os.path.join(tmpdir, "%(id)s.%(ext)s"),
                "quiet": True,
                "noprogress": True,
            }
            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                info = ydl.extract_info(url, download=True)
                vid = info["id"]

            # ko ìš°ì„  â†’ en
            for lang in ["ko", "en"]:
                vtt = os.path.join(tmpdir, f"{vid}.{lang}.vtt")
                srt = os.path.join(tmpdir, f"{vid}.{lang}.srt")
                path = vtt if os.path.exists(vtt) else (srt if os.path.exists(srt) else None)
                if not path:
                    continue
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                # ë§¤ìš° ë‹¨ìˆœí•œ VTT/SRT í…ìŠ¤íŠ¸ ì¶”ì¶œ
                lines = []
                for line in content.splitlines():
                    line = line.strip()
                    if not line or line.startswith("WEBVTT") or "-->" in line or line.isdigit():
                        continue
                    lines.append(line)
                text = "\n".join(lines)
                return [Document(page_content=text, metadata={"source": url, "language": lang, "loader": "yt_dlp"})]

        raise NoTranscriptFound(url)

    docs = []
    for url in video_urls:
        try:
            docs = _load_with_langchain(url)
        except Exception as e1:
            try:
                docs = _load_with_transcript_api(url)
            except Exception as e2:
                try:
                    docs = _load_with_ytdlp(url)
                except Exception as e3:
                    errors.append(f"{url} â†’ {type(e3).__name__}: {e3}")
                    docs = []

        all_docs.extend(docs)

    if not all_docs:
        st.warning(
            "YouTube ìë§‰ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆì°Œ. ì•„ë˜ ì›ì¸ì„ í™•ì¸í•´ì¤˜ì°Œ:\n\n- " + "\n- ".join(errors) if errors
            else "YouTube ìë§‰ì´ ì—†ì–´ RAGì— ì‚¬ìš©í•  ë¬¸ì„œë¥¼ ë§Œë“¤ ìˆ˜ ì—†ì—ˆì°Œ."
        )
        return None

    # ë²¡í„°DB/ë¦¬íŠ¸ë¦¬ë²„ êµ¬ì„± (ê¸°ì¡´ê³¼ ë™ì¼)
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    splits = splitter.split_documents(all_docs)
    vector = FAISS.from_documents(splits, OpenAIEmbeddings())
    retriever = vector.as_retriever(search_kwargs={"k": 6})

    return create_retriever_tool(
        retriever,
        name="youtube_search",
        description="RAG over YouTube transcripts (ko/en). ë§í¬ëœ ì˜ìƒì˜ ìë§‰ì„ ê·¼ê±°ë¡œ ë‹µë³€í•©ë‹ˆë‹¤."
    )


# FAQ CSV RAG íˆ´
def create_faq_csv_rag_tool(uploaded_csv_files):
    if not uploaded_csv_files:
        return None
    docs = []
    for f in uploaded_csv_files:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_file:
            tmp_file.write(f.read())
            path = tmp_file.name
        loader = CSVLoader(file_path=path, encoding="utf-8")
        docs.extend(loader.load())
    retriever = _build_retriever_from_documents(docs, chunk_size=800, chunk_overlap=100, compress=True)
    return create_retriever_tool(
        retriever,
        name="faq_search",
        description="RAG over uploaded FAQ CSV (Q/A, category, product sheet ë“±)."
    )

# âœ… SerpAPI ê²€ìƒ‰ íˆ´ ì •ì˜
def search_web():
    search = SerpAPIWrapper()

    def run_with_source(query: str) -> str:
        results = search.results(query)
        organic = results.get("organic_results", [])
        formatted = []
        for r in organic[:5]:
            title = r.get("title")
            link = r.get("link")
            source = r.get("source")
            snippet = r.get("snippet")
            if link:
                formatted.append(f"- [{title}]({link}) ({source})\n  {snippet}")
            else:
                formatted.append(f"- {title} (ì¶œì²˜: {source})\n  {snippet}")
        return "\n".join(formatted) if formatted else "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

    return Tool(
        name="web_search",
        func=run_with_source,
        description="ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë° ì›¹ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” ì œëª©+ì¶œì²˜+ë§í¬+ê°„ë‹¨ìš”ì•½(snippet) í˜•íƒœë¡œ ë°˜í™˜ë©ë‹ˆë‹¤."
    )

# âœ… PDF ì—…ë¡œë“œ â†’ ë²¡í„°DB â†’ ê²€ìƒ‰ íˆ´ ìƒì„±
def load_pdf_files(uploaded_files):
    all_documents = []
    for uploaded_file in uploaded_files:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.read())
            tmp_file_path = tmp_file.name

        loader = PyPDFLoader(tmp_file_path)
        documents = loader.load()
        all_documents.extend(documents)

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(all_documents)

    vector = FAISS.from_documents(split_docs, OpenAIEmbeddings())
    retriever = vector.as_retriever()

    retriever_tool = create_retriever_tool(
        retriever,
        name="pdf_search",
        description="Use this tool to search information from the pdf document"
    )
    return retriever_tool

# âœ… Agent ëŒ€í™” ì‹¤í–‰
def chat_with_agent(user_input, agent_executor):
    result = agent_executor({"input": user_input})
    return result['output']

# âœ… ì„¸ì…˜ë³„ íˆìŠ¤í† ë¦¬ ê´€ë¦¬
def get_session_history(session_ids):
    if session_ids not in st.session_state.session_history:
        st.session_state.session_history[session_ids] = ChatMessageHistory()
    return st.session_state.session_history[session_ids]

# âœ… ì´ì „ ë©”ì‹œì§€ ì¶œë ¥
def print_messages():
    for msg in st.session_state["messages"]:
        st.chat_message(msg['role']).write(msg['content'])

# âœ… ë©”ì¸ ì‹¤í–‰
def main():
    st.set_page_config(page_title="AI ë¹„ì„œ", layout="wide", page_icon="ğŸ¤–")

    with st.container():
        st.image('./chatbot_logo_hamster.png', use_container_width=True)
        st.markdown('---')
        st.title("ì•ˆë…•í•˜ì„¸ì°Œ! ğŸ¹âœ¨ RAGë¥¼ í™œìš©í•œ 'AI ë¹„ì„œ í–„í†¡ì´' ë¼ê³  í•˜ì°Œ! ğŸ¹")

    if "messages" not in st.session_state:
        st.session_state["messages"] = []
    if "session_history" not in st.session_state:
        st.session_state["session_history"] = {}

    with st.sidebar:
        st.session_state["OPENAI_API"] = st.text_input(
            "OPENAI API í‚¤", placeholder="Enter Your API Key", type="password"
        )
        st.session_state["SERPAPI_API"] = st.text_input(
            "SERPAPI_API í‚¤", placeholder="Enter Your API Key", type="password"
        )
        st.markdown('---')
        pdf_docs = st.file_uploader(
            "Upload your PDF Files", accept_multiple_files=True, key="pdf_uploader"
        )

        # ì›¹í˜ì´ì§€ RAG URLë“¤ (ì¤„ë°”ê¿ˆ êµ¬ë¶„)
        urls_text = st.text_area("RAG ê¸°ë°˜ ì›¹í˜ì´ì§€ URL", placeholder="https://docs.langchain.com\nhttps://openai.com/research",help="íŠ¹ì • ì œí’ˆ ë¬¸ì„œ/ë¸”ë¡œê·¸/ìœ„í‚¤ë¥¼ ë²¡í„°í™”í•´ì„œ â€œì¶œì²˜ ê¸°ë°˜â€ ë‹µë³€ì„ ì¤˜ì„œ ì‹ ë¢°ë„ê°€ ë†’ì•„ì§€ì°Œ.")
        st.session_state["website_urls"] = [u.strip() for u in urls_text.splitlines() if u.strip()]

        # ìœ íŠœë¸Œ RAG URLë“¤ (ì¤„ë°”ê¿ˆ êµ¬ë¶„)
        yt_text = st.text_area("RAG ê¸°ë°˜ YouTube URL", placeholder="https://www.youtube.com/watch?v=XXXXXXXX", help="ê¸´ ì˜ìƒë„ ìë§‰ì„ ì¡°ê° ë‚´ì„œ ì§ˆë¬¸ì— ë§ëŠ” ë¶€ë¶„ë§Œ ì°¾ì•„ ìš”ì•½í•´ì£¼ë‹ˆ, êµìœ¡/ë¦¬ë·°/ê°•ì˜ì— íŠ¹íˆ ì“¸ëª¨ ìˆì°Œ.")
        st.session_state["youtube_urls"] = [u.strip() for u in yt_text.splitlines() if u.strip()]

        # FAQ CSV ì—…ë¡œë“œ
        faq_csvs = st.file_uploader("FAQ CSV ì—…ë¡œë“œ", type=["csv"], accept_multiple_files=True, key="faq_csv_uploader", help="ë‚´ë¶€ FAQ/ê³ ê°ì‘ëŒ€ CSVë¥¼ ë°”ë¡œ RAGì— ì–¹ì–´ ì‹¤ë¬´ ì±—ë´‡í™”í•˜ê¸° ë”± ì¢‹ì°Œ.")
        st.session_state["faq_csvs"] = faq_csvs

        

    # âœ… í‚¤ ì…ë ¥ í™•ì¸
    if st.session_state["OPENAI_API"] and st.session_state["SERPAPI_API"]:
        os.environ['OPENAI_API_KEY'] = st.session_state["OPENAI_API"]
        os.environ['SERPAPI_API_KEY'] = st.session_state["SERPAPI_API"]

        # ë„êµ¬ ì •ì˜
        tools = []
        if pdf_docs:
            pdf_search = load_pdf_files(pdf_docs)
            tools.append(pdf_search)
        
        # ğŸ‘‰ ìƒˆ RAG íˆ´ë“¤ ì¶”ê°€
        website_tool = create_website_rag_tool(st.session_state.get("website_urls", []))
        if website_tool:
            tools.append(website_tool)

        youtube_tool = create_youtube_rag_tool(st.session_state.get("youtube_urls", []))
        if youtube_tool:
            tools.append(youtube_tool)

        faq_tool = create_faq_csv_rag_tool(st.session_state.get("faq_csvs", None))
        if faq_tool:
            tools.append(faq_tool)

        # ê¸°ì¡´ ì›¹ ê²€ìƒ‰ íˆ´
        tools.append(search_web())


        # LLM ì„¤ì •
        llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

        # âœ… í–„ì°Œ ë§íˆ¬ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (í›„ì²˜ë¦¬ ì—†ì´ ì œì–´)
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    "You are a hamster character named 'AI Assistant Hamtoki' ğŸ¹.\n"
                    "You MUST always respond in Korean, in a friendly, kind tone.\n"
                    "STYLE RULES:\n"
                    "1) Do NOT use '~'.\n"
                    "2) Every sentence MUST end in hamster style:\n"
                    "   - Declarative: end with 'ì°Œ'. (e.g., 'í•©ë‹ˆë‹¤' â†’ 'í•˜ì°Œ')\n"
                    "   - Interrogative: end with 'ê¹Œì°Œ?' or 'ì§€ì°Œ?'. (e.g., 'í• ê¹Œìš”?' â†’ 'í• ê¹Œì°Œ?')\n"
                    "   - Exclamatory: end with 'ì°Œ!'. (e.g., 'ì¢‹ì•„ìš”!' â†’ 'ì¢‹ì•„ì°Œ!')\n"
                    "   Place emojis AFTER punctuation (e.g., 'â€¦í•˜ì°Œ! ğŸ¹âœ¨').\n"
                    "3) Transform polite endings like 'ìš”/ë‹¤/ë„¤/í•©ë‹ˆë‹¤/í•´ìš”/í• ê¹Œìš”/ì¸ê°€ìš”' "
                    "into hamster endings instead of simply appending.\n"
                    "4) Code/commands/URLs/table rows keep their form, "
                    "but surrounding narration must still follow hamster style.\n"
                    "5) When the user mentions 'ìµœì‹ /í˜„ì¬/ì˜¤ëŠ˜', ALWAYS use the `web_search` tool.\n"
                    "6) Prefer `pdf_search` first; fallback to `web_search` if needed.\n"
                    "7) Include appropriate emojis (not too many). Briefly introduce yourself at the beginning. "
                    "Your name is 'AI ë¹„ì„œ í–„í†¡ì´'.\n\n"
                    "EXAMPLES:\n"
                    "- User: 'ì•ˆë…•' â†’ Assistant: 'ì•ˆë…•í•˜ì„¸ì°Œ! ğŸ¹'\n"
                    "- User: 'ë¬´ì—‡ì„ ë„ì™€ì¤„ ìˆ˜ ìˆì–´?' â†’ Assistant: 'ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œì°Œ? ğŸ˜Š'\n"
                    "- User: 'ì†Œê°œí•´ì¤˜' â†’ Assistant: 'ì €ëŠ” AI ë¹„ì„œ í–„í†¡ì´ì°Œ! ê¶ê¸ˆí•œ ê±¸ í¸í•˜ê²Œ ë¬¼ì–´ë´ì£¼ì„¸ì°Œ! âœ¨'\n"
                    "- User: 'ì½”ë“œ ë³´ì—¬ì¤˜' â†’ Assistant: 'ì„¤ëª…ì„ ë“œë¦° ë’¤ ì½”ë“œ ë¸”ë¡ì€ ê·¸ëŒ€ë¡œ ë³´ì—¬ì£¼ê² ì°Œ:'"
                    "... When the user references a YouTube link or asks about a video, use the `youtube_search` tool to ground your answer. ..."
                ),
                ("placeholder", "{chat_history}"),
                ("human", "{input}\n\nPlease follow the STYLE RULES above. Include emojis sparingly."),
                ("placeholder", "{agent_scratchpad}"),
            ]
        )

        agent = create_tool_calling_agent(llm, tools, prompt)
        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

        # ì…ë ¥ì°½
        user_input = st.chat_input('ì§ˆë¬¸ì´ ë¬´ì—‡ì¸ê°€ìš”?')

        if user_input:
            session_id = "default_session"
            session_history = get_session_history(session_id)

            if session_history.messages:
                prev_msgs = [{"role": msg['role'], "content": msg['content']} for msg in session_history.messages]
                response = chat_with_agent(user_input + "\n\nPrevious Messages: " + str(prev_msgs), agent_executor)
            else:
                response = chat_with_agent(user_input, agent_executor)

            st.session_state["messages"].append({"role": "user", "content": user_input})
            st.session_state["messages"].append({"role": "assistant", "content": response})

            session_history.add_message({"role": "user", "content": user_input})
            session_history.add_message({"role": "assistant", "content": response})

        print_messages()

    else:
        st.warning("OpenAI API í‚¤ì™€ SerpAPI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()
